{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the Meaning and Use of Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "Let's start with eigendecomposition. Remember PCA?\n",
    "\n",
    "Any *non-defective* and *square* matrix $A$ has an eigendecomposition:\n",
    "\n",
    "$\\large A = Q\\Lambda Q^{-1}$,\n",
    "\n",
    "where:\n",
    "\n",
    "- the columns of $Q$ are the eigenvectors of $A$, and\n",
    "- $\\Lambda$ is a diagonal matrix whose non-zero entries are the eigenvalues of $A$.\n",
    "\n",
    "Eigendecompositions have *many* practical applications.\n",
    "\n",
    "But since not all matrices are square, not all matrices have eigendecompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "However, given a non-square matrix $R$, we can construct a square matrix by simply calculating $RR^T$ or $R^TR$.\n",
    "\n",
    "(The 'T' superscript indicates that the relevant matrix is *transposed*, i.e. its rows and columns are switched. So for example the transpose of $\\begin{bmatrix} 0 & 1 & 2 \\\\ 3 & 4 & 5 \\\\ 6 & 7 & 8 \\end{bmatrix}$ is $\\begin{bmatrix} 0 & 3 & 6 \\\\ 1 & 4 & 7 \\\\ 2 & 5 & 8 \\end{bmatrix}$.)\n",
    "\n",
    "Moreover, _any_ matrix $A$ has a ***singular value decomposition***, i.e. a factorization in the form:\n",
    "\n",
    "$\\large A = U\\Sigma V^T$,\n",
    "\n",
    "where\n",
    "\n",
    "- $\\Sigma$ is diagonal if square and otherwise \"pseudo-diagonal\" (a diagonal matrix sitting on top of zeroes) with real, non-negative entries; and\n",
    "- $U$ and $V$ are orthogonal.\n",
    "\n",
    "A matrix $Q$ is orthogonal if its columns are mutually orthogonal and normalized to lengths of 1. This guarantees that, for orthogonal $Q$, $Q^TQ = I$. Thus, $Q^T = Q^T(QQ^{-1}) = (Q^TQ)Q^{-1}$, so $Q^T = Q^{-1}$.\n",
    "\n",
    "Note also that, if $V$ is orthogonal, then so is $V^{-1}$:\n",
    "\n",
    "$(V^{-1})^T = (V^T)^T$ <br/>\n",
    "$(V^{-1})^T = V$ <br/>\n",
    "$(V^{-1})^T = (V^{-1})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (this text adapted from [Inderjit Dhillon]( http://www.cs.utexas.edu/users/inderjit/courses/dm2009/LinearAlgebraBackground.pdf)):\n",
    "\n",
    "Using the singular value decomposition of $A$, we have:\n",
    "\n",
    "$\\large AA^T = U\\Sigma V^T\\times(U\\Sigma V^T)^T$ <br/>\n",
    "$\\large AA^T = U\\Sigma V^T\\times V\\Sigma^TU^T$ <br/>\n",
    "$\\large AA^T = U\\Sigma I\\Sigma U^T$ <br/>\n",
    "$\\large AA^T = U\\Sigma^2U^T$ <br/>\n",
    "Here we have an eigendecomposition of $AA^T$ in terms of the SVD of $A$!\n",
    "\n",
    "In particular:\n",
    "\n",
    "the eigenvectors of $AA^T$ are the columns of $U$; and <br/>\n",
    "the eigenvalues of $AA^T$ are the squares of the singular values of $A$. <br/>\n",
    "\n",
    "Similarly, $\\large A^TA = V\\Sigma^2V^T$.\n",
    "\n",
    "And so:\n",
    "\n",
    "the eigenvectors of $A^TA$ are the columns of $V$; and <br/>\n",
    "the eigenvalues of $A^TA$ are the squares of the singular values of $A$.<br/>\n",
    "\n",
    "Put another way: The singular values of A are the non-negative square roots of the eigenvalues of $AA^T$ or $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again (see [this page](https://math.stackexchange.com/questions/2152751/why-does-the-eigenvalues-of-aat-are-the-squares-of-the-singular-values-of-a)):\n",
    "\n",
    "Since: <br/> $\\large AA^T = U\\Sigma^2U^T$, <br/> we have that <br/> $\\large AA^TU = U\\Sigma^2$ (since $U$ is orthogonal), <br/> which says that $AA^T$ multiplied by a vector (let's choose $U_i$, a column of $U$) yields $U$ multiplied by a scalar, namely $\\sigma^2_i$. I.e. the squares of the singular values of $A$ are the (non-zero) eigenvalues of $A^TA$ (or $AA^T$).\n",
    "\n",
    "### Eigenvalues of $AA^T$ and $A^TA$\n",
    "Why do $AA^T$ and $A^TA$ have the same eigenvalues?\n",
    "\n",
    "Let $\\lambda$ be a (non-zero) eigenvalue of $A^TA$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$A^TAx = \\lambda x$.\n",
    "\n",
    "Now let $Ax = y$. So we have:\n",
    "\n",
    "$A^Ty = \\lambda x$. If we left-multiply by $A$, we have:\n",
    "\n",
    "$AA^Ty = A\\lambda x = \\lambda Ax = \\lambda y$, which is to say that $\\lambda$ is an eigenvalue of $AA^T$.\n",
    "\n",
    "(To show the reverse, just let $B = A^T$.)\n",
    "\n",
    "For more, see [this post](https://math.stackexchange.com/questions/1087064/non-zero-eigenvalues-of-aat-and-ata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonalization vs. SVD\n",
    "\n",
    "This shows that the eigen- and the singular value decompositions are intimately related!\n",
    "\n",
    "The SVD has many uses! Check out [this book chapter](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD and Dimensionality Reduction\n",
    "\n",
    "The SVD, much like PCA, can be used to reduce the dimensionality of your data.\n",
    "\n",
    "Recall how PCA works: We start with an eigendecomposition of our covariance matrix. We then order our eigenvectors by the size of their corresponding eigenvalues. Eigenvectors with large eigenvalues explain more of the variance in our dataset, and so we define our principal components accordingly.\n",
    "\n",
    "The situation is much the same with the SVD. The singular vectors that correspond to larger singular values capture more of the variance in our data. And, just as with PCA, we can often capture a large percentage of the variance by taking a relatively small number of singular vectors, throwing out the ones that correspond to small singular values. [Here](https://rpubs.com/Tanmay007/svd) is a good example of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD in Python\n",
    "\n",
    "Let's show that the squares of the singular values of a non-square matrix $A$ are equal to the eigenvalues of $A^TA$ (or $AA^T$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "A = np.random.rand(5, 3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using np.linalg.svd()\n",
    "\n",
    "np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.linalg.svd()` returns a triple, unsurprisingly: The left singular vectors (\"U\"), the singular values, and the (transpose of the) right singular vectors (\"V\").\n",
    "\n",
    "We can re-create A by multiplying these components together. But we'll first have to turn the array of singular values into $\\Sigma$, so we'll use `np.diag()` together with `np.vstack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vT = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.vstack([np.diag(s), [[0, 0, 0], [0, 0, 0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should reproduce the matrix A\n",
    "\n",
    "u.dot(sigma).dot(vT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the eigendecomposition of $AA^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(A.dot(A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(A.dot(A.T))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two eigenvalues are really equal to *zero*, which of course can often confuse computers. Squaring the singular values of $A$ should yield the same list (without the zeroes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.svd(A)[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Least-Squares Problem\n",
    "\n",
    "The singular value decomposition can be used to solve a least-squares problem quickly. Let's create such a problem.\n",
    "\n",
    "### Comparison to the matrix-vector equation, $M\\vec{x} = \\vec{b}$\n",
    "\n",
    "Suppose we have an exact equation, $M\\vec{x} = \\vec{b}$.\n",
    "\n",
    "In that case $M$ is square, and the solution to the equation is $x = M^{-1}b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "M = np.random.rand(5, 5)\n",
    "b = np.random.rand(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linalg.inv(M).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducing the vector b\n",
    "\n",
    "M.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Problem\n",
    "\n",
    "But of course the typical DS situation is that we have not an exact equation to solve but rather an optimization to perform. So let's now imagine that $A$ has more rows than columns.\n",
    "\n",
    "If we need some warm and fuzzy familiarity, we could throw this all into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack([A, b]),\n",
    "             columns=['pred1', 'pred2', 'pred3', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating the columns of $A$ as our predictors and $b$ as our target, the answer to this least-squares problem turns out to be $A^+\\vec{b}$, where $A^+$ is the *pseudo-inverse* of $A$. The formula for the pseudo-inverse if $(A^TA)^{-1}A^T$, and the idea behind it is to generalize the notion of an inverse to non-square matrices. The pseudo-inverse reduces to the inverse in the case of square matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.random.rand(100, 100)\n",
    "np.allclose(np.linalg.inv(mat), np.linalg.pinv(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have $A = U\\Sigma V^T$, then $A^+ = V\\Sigma^+U^T$. Numpy has a pseudo-inverse function, `np.linalg.pinv()`, which we could use directly, rather than first constructing the SVD. But because the decomposed equation involves only the pseudo-inverse of a (pseudo-) diagonal matrix, the SVD route can be much *faster*. **This is the real point of using the SVD in calculating least-squares solutions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this site](https://math.stackexchange.com/questions/974193/why-does-svd-provide-the-least-squares-and-least-norm-solution-to-a-x-b) for a proof of the least-squares solution. For more on the pseudo-inverse, see [here](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the least-squares solution using our SVD components:\n",
    "\n",
    "vT.T.dot(np.linalg.pinv(sigma)).dot(u.T).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking against sklearn's LinearRegression():\n",
    "\n",
    "LinearRegression(fit_intercept=False).fit(A, b).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `LinearRegression()` uses SVD under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit vT.T.dot(np.linalg.pinv(sigma)).dot(u.T).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit LinearRegression(fit_intercept=False).fit(A, b).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.pinv(A).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our small sample matrix, `sklearn`'s version actually takes longer. But let's try a much larger matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(10000, 100)\n",
    "target = np.random.rand(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.pinv(X).dot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit LinearRegression(fit_intercept=False).fit(X, target).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the SVD (as `sklearn`'s model-fitting procedure does) makes noticeable savings in time here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD in Rec Systems\n",
    "\n",
    "Since the SVD solves the least-squares problem, it can be used in ALS-style recommenders. But whereas ALS is generally used for implicit ratings, the SVD is more useful for explicit ratings, such as exist in the [MovieLens](https://grouplens.org/datasets/movielens/) datset.\n",
    "\n",
    "Much of the following code comes from [this](https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/) helpful article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ratings.dat',\n",
    "                   names=['user_id', 'movie_id', 'rating', 'time'],\n",
    "                   engine='python', delimiter='::')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.groupby('user_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('movie_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('data/movies.dat',\n",
    "                     names=['movie_id', 'title', 'genre'],\n",
    "                    engine='python', delimiter='::')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('data/users.dat',\n",
    "                    names=['user_id', 'gender', 'age',\n",
    "                          'occupation', 'ZIP code'],\n",
    "                   engine='python', delimiter='::')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the documentation for how the `age` and `occupation` columns are coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the rating matrix (rows as movies, columns as users)\n",
    "\n",
    "ratings_mat = np.ndarray(\n",
    "    shape=(np.max(data['movie_id']), np.max(data['user_id'])),\n",
    "    dtype=np.uint8)\n",
    "ratings_mat[data['movie_id']-1, data['user_id']-1] = data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['movie_id'][0], data['user_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_mat[1192, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_mat[1096, 6039]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the matrix (subtract mean movie rating off)\n",
    "\n",
    "normalized_mat = ratings_mat - np.asarray([(np.mean(ratings_mat, axis=1))]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this type of \"normalization\" see [this post](https://stats.stackexchange.com/questions/31096/how-do-i-use-the-svd-in-collaborative-filtering) and [this article](http://nicolas-hug.com/blog/matrix_facto_3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Singular Value Decomposition (SVD)\n",
    "\n",
    "A = normalized_mat.T / np.sqrt(ratings_mat.shape[0] - 1)\n",
    "U, S, V = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cosine_similarity(data, movie_id, top_n=10):\n",
    "    \"\"\"This function calculates cosine similarity and returns the\n",
    "    top n highest scores for user-defined n.\"\"\"\n",
    "    index = movie_id - 1 # Movie id starts from 1 in the dataset\n",
    "    movie_row = data[index, :]\n",
    "    \n",
    "    # This is just a shortcut for calculating vector norms:\n",
    "    magnitude = np.sqrt(np.einsum('ij, ij -> i', data, data))\n",
    "    similarity = np.dot(movie_row, data.T) / (magnitude[index] * magnitude)\n",
    "    sort_indices = np.argsort(-similarity)\n",
    "    return sort_indices[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_movies(movie_data, movie_id, top_indices):\n",
    "    \"\"\"This function prints out the movies corresponding to\n",
    "    a set of indices\"\"\"\n",
    "    print(f\"\"\"Recommendations for\n",
    "    {movie_data[movie_data['movie_id'] == movie_id]['title'][0]}: \\n\"\"\")\n",
    "    for id in top_indices + 1:\n",
    "        print(movie_data[movie_data.movie_id == id].title.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-principal components to represent movies,\n",
    "# movie_id to find recommendations, top_n print n results      \n",
    "\n",
    "k = 50\n",
    "movie_id = 1 # (getting an id from movies.dat)\n",
    "top_n = 10\n",
    "sliced = V.T[:, :k] # representative data\n",
    "indices = top_cosine_similarity(sliced, movie_id, top_n)\n",
    "\n",
    "# Printing the top N similar movies\n",
    "print_similar_movies(movie_data, movie_id, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `surprise` package\n",
    "`pip install surprise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code taken from https://surprise.readthedocs.io/en/stable/\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
